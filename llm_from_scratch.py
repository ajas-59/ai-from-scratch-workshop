# -*- coding: utf-8 -*-
"""llm_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yHRDYL8vmIXjzVKJhYy8EwOJ7NSgad0e
"""

!pip install langchain-google-genai langchain-community faiss-cpu sentence-transformers

from langchain_google_genai import GoogleGenerativeAI

google_api_key = input("Key")

llm = GoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=google_api_key)

"""changing temperature adds randomness"""

response = llm.invoke("Say hello to this workshop participants")

print(response)

def raw_ai():
  question = input("Ask Gemini anything : ")
  response = llm.invoke(question)
  print(response)

raw_ai()

questions = [
    "Explain Machine Learning",
    "What's this machine learning thing"
]

for q in questions:
  print(f"Question: {q}")
  print(f"Answer: {llm.invoke(q)[:150]}")

"""A more organised llm to prompt"""

from langchain_google_genai import GoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = PromptTemplate(input_variables=["topic"],
               template="""Explain {topic} in simple terms with an example :

               Format :
               - Keep it under 100 words
               - Use an analogy
               -End with a practical example

               Topic : {topic}
               Explanation: """
               )
filled_prompt = prompt.format(topic="Machine Learning")
print(filled_prompt)

# give prompt
# connect to llm + get response
# format response + give it as output


chain = prompt | llm | StrOutputParser()

topics = ['blokchain', 'AI agent', 'Internet Model']

for t in topics:
  print(f'\n Explaining: {t}')

  result = chain.invoke({'topic': t})

  print(f'Response: {result}')

company_questions = [
    'What is your return policy?',
    'What are your shipping costs?'
                    ]

for company_q in company_questions:
  response = llm.invoke(company_q)
  print(f'\nQuestion: {company_q}')
  print(f'Answer: {response[:150]}')

"""Now giving the AI a handbook = extra knowledge provided"""

documents = ['We offer 30-day returns with receipts',
             "Free shipping for orders over $50",
             "We accept all major credit cards",
             'We accept all major credit cards'
             ]

"""Rule based system : take question + find closest document to that question

String representation in numbers

documents = [4, 3, 1, 2]

question = [1, 9, 6, 3]

find closest embedding
"""

from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name = 'all-miniLM-L6-v2')

sample_text = ['return policy', 'refund policy', 'weather forcast']

sample_embeddings = embeddings.embed_documents(sample_text)

print([len(text) for text in sample_text])

print([len(embd) for embd in sample_embeddings])

from langchain.vectorstores import FAISS
from langchain.schema import Document

documents

docs = [Document(page_content=doc) for doc in documents]

docs

vectorstore = FAISS.from_documents(docs, embeddings)

vectorstore

test_query = 'refund policy'

relevant_document = vectorstore.similarity_search(test_query, k=1)

print(relevant_document)

[doc.page_content for doc in relevant_document]

'\n'.join([doc.page_content for doc in relevant_document])

print(f'\n{relevant_document[0].page_content}')

"""looked at cosine similarity"""

def ask_rag(question):
  relevant_document = vectorstore.similarity_search(question, k=2)

  context = '\n'.join([doc.page_content for doc in relevant_document])

  rag_prompt = f"""

  Based on this comany information:
  {context}

  Question: {question}

  Please provide a helpful answer based on the company information provided.

  Answer:
  """

  response = llm.invoke(rag_prompt)

  return response

question = "What is your return policy?"

ask_rag(question)